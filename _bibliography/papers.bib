---
---
@INPROCEEDINGS{9524757, abbr = {SOCC}, author={Yuan, Geng and Ma, Xiaolong and Lin, Sheng and Li, Zhengang and Deng, Jieren and Ding, Caiwen},  booktitle={2020 IEEE 33rd International System-on-Chip Conference (SOCC)},   title={A DNN Compression Framework for SOT-MRAM-based Processing-In-Memory Engine},   year={2020},  volume={},  number={},  pages={37-42},  doi={10.1109/SOCC49529.2020.9524757}}

@INPROCEEDINGS{9643440, abbr = {ICCAD},  author={Manu, Daniel and Sheng, Yi and Yang, Junhuan and Deng, Jieren and Geng, Tong and Li, Ang and Ding, Caiwen and Jiang, Weiwen and Yang, Lei},  booktitle={2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)},   title={FL-DISCO: Federated Generative Adversarial Network for Graph-based Molecule Drug Discovery: Special Session Paper},   year={2021},  volume={},  number={},  pages={1-7},  doi={10.1109/ICCAD51958.2021.9643440}}

@INPROCEEDINGS{9474235,
  abbr={DATE}, 
  author={Yuan, Geng and Behnam, Payman and Cai, Yuxuan and Shafiee, Ali and Fu, Jingyan and Liao, Zhiheng and Li, Zhengang and Ma, Xiaolong and Deng, Jieren and Wang, Jinhui and Bojnordi, Mahdi and Wang, Yanzhi and Ding, Caiwen},
  booktitle={2021 Design, Automation   Test in Europe Conference   Exhibition (DATE)}, 
  title={TinyADC: Peripheral Circuit-aware Weight Pruning Framework for Mixed-signal DNN Accelerators}, 
  year={2021},
  volume={},
  number={},
  pages={926-931},
  doi={10.23919/DATE51398.2021.9474235}}

@misc{https://doi.org/10.48550/arxiv.2009.01867,
  abbr={NeurIPS SpicyFL},
  doi = {10.48550/ARXIV.2009.01867},
  
  url = {https://arxiv.org/abs/2009.01867},
  
  author = {Lin, Sheng and Wang, Chenghong and Li, Hongjia and Deng, Jieren and Wang, Yanzhi and Ding, Caiwen},
  
  keywords = {Cryptography and Security (cs.CR), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ESMFL: Efficient and Secure Models for Federated Learning},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2009.06228,
  doi = {10.48550/ARXIV.2009.06228},
  
  url = {https://arxiv.org/abs/2009.06228},
  
  author = {Wang, Yijue and Deng, Jieren and Guo, Dan and Wang, Chenghong and Meng, Xianrui and Liu, Hang and Ding, Caiwen and Rajasekaran, Sanguthevar},
  
  keywords = {Machine Learning (cs.LG), Cryptography and Security (cs.CR), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SAPAG: A Self-Adaptive Privacy Attack From Gradients},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{wu2021novel,
  abbr={BIB},
  title={A novel privacy-preserving federated genome-wide association study framework and its application in identifying potential risk variants in ankylosing spondylitis},
  author={Wu, Xin and Zheng, Hao and Dou, Zuochao and Chen, Feng and Deng, Jieren and Chen, Xiang and Xu, Shengqian and Gao, Guanmin and Li, Mengmeng and Wang, Zhen and others},
  journal={Briefings in Bioinformatics},
  volume={22},
  number={3},
  pages={bbaa090},
  year={2021},
  publisher={Oxford University Press},
  selected={true},
  html ={https://academic.oup.com/bib/article/22/3/bbaa090/5860679?login=true},
  abstract = "Genome-wide association studies (GWAS) have been widely used for identifying potential risk variants in various diseases. A statistically meaningful GWAS typically requires a large sample size to detect disease-associated single nucleotide polymorphisms (SNPs). However, a single institution usually only possesses a limited number of samples. Therefore, cross-institutional partnerships are required to increase sample size and statistical power. However, cross-institutional partnerships offer significant challenges, a major one being data privacy. For example, the privacy awareness of people, the impact of data privacy leakages and the privacy-related risks are becoming increasingly important, while there is no de-identification standard available to safeguard genomic data sharing. In this paper, we introduce a novel privacy-preserving federated GWAS framework (iPRIVATES). Equipped with privacy-preserving federated analysis, iPRIVATES enables multiple institutions to jointly perform GWAS analysis without leaking patient-level genotyping data. Only aggregated local statistics are exchanged within the study network. In addition, we evaluate the performance of iPRIVATES through both simulated data and a real-world application for identifying potential risk variants in ankylosing spondylitis (AS). The experimental results showed that the strongest signal of AS-associated SNPs reside mostly around the human leukocyte antigen (HLA) regions. The proposed iPRIVATES framework achieved equivalent results as traditional centralized implementation, demonstrating its great potential in driving collaborative genomic research for different diseases while preserving data privacy."
}

@inproceedings{deng-etal-2021-tag-gradient,
    abbr={EMNLP},
    title = "{TAG}: Gradient Attack on Transformer-based Language Models",
    author = "Deng, Jieren  and
      Wang, Yijue  and
      Li, Ji  and
      Wang, Chenghong  and
      Shang, Chao  and
      Liu, Hang  and
      Rajasekaran, Sanguthevar  and
      Ding, Caiwen",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.305",
    doi = "10.18653/v1/2021.findings-emnlp.305",
    pages = "3600--3610",
    abstract = "Although distributed learning has increasingly gained attention in terms of effectively utilizing local devices for data privacy enhancement, recent studies show that publicly shared gradients in the training process can reveal the private training data (gradient leakage) to a third-party. We have, however, no systematic understanding of the gradient leakage mechanism on the Transformer based language models. In this paper, as the first attempt, we formulate the gradient attack problem on the Transformer-based language models and propose a gradient attack algorithm, TAG, to reconstruct the local training data. Experimental results on Transformer, TinyBERT4, TinyBERT6 BERT{\_}BASE, and BERT{\_}LARGE using GLUE benchmark show that compared with DLG, TAG works well on more weight distributions in reconstructing training data and achieves 1.5x recover rate and 2.5x ROUGE-2 over prior methods without the need of ground truth label. TAG can obtain up to 90{\%} data by attacking gradients in CoLA dataset. In addition, TAG is stronger than previous approaches on larger models, smaller dictionary size, and smaller input length. We hope the proposed TAG will shed some light on the privacy leakage problem in Transformer-based NLP models.",
    selected={true},
    pdf = {https://aclanthology.org/2021.findings-emnlp.305.pdf}
}


@inproceedings{wang-etal-2021-secure,
    abbr={EMNLP},
    title = "A Secure and Efficient Federated Learning Framework for {NLP}",
    author = " Deng, Jieren and
      Wang, Chenghong  and
      Meng, Xianrui  and
      Wang, Yijue  and
      Li, Ji  and
      Lin, Sheng  and
      Han, Shuo  and
      Miao, Fei  and
      Rajasekaran, Sanguthevar  and
      Ding, Caiwen",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.606",
    doi = "10.18653/v1/2021.emnlp-main.606",
    pages = "7676--7682",
    abstract = "In this work, we consider the problem of designing secure and efficient federated learning (FL) frameworks for NLP. Existing solutions under this literature either consider a trusted aggregator or require heavy-weight cryptographic primitives, which makes the performance significantly degraded. Moreover, many existing secure FL designs work only under the restrictive assumption that none of the clients can be dropped out from the training protocol. To tackle these problems, we propose SEFL, a secure and efficient federated learning framework that (1) eliminates the need for the trusted entities; (2) achieves similar and even better model accuracy compared with existing FL designs; (3) is resilient to client dropouts.",
    selected={true},
    pdf = {https://aclanthology.org/2021.emnlp-main.606.pdf}
}



